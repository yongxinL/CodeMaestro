# Token Estimation and Budget Management

**CodeMaestro v1.0.0**
**Purpose:** Guide for estimating, tracking, and managing token usage throughout development lifecycle
**Target:** Software Architects (Phase 2), Senior Developers (Phase 3), QA Leads (Phase 4)

---

## Overview

Token estimation helps predict and manage the computational resources (LLM API tokens) required to complete development tasks using AI assistance. Accurate token estimation enables:

- **Budget Planning:** Allocate token budgets per task/milestone
- **Cost Control:** Prevent unexpected API costs
- **Session Management:** Optimize context window usage
- **Performance Tracking:** Measure actual vs estimated token efficiency
- **Iteration Planning:** Inform future estimations based on historical data

---

## Key Concepts

### What are Tokens?

Tokens are the units of text processed by Large Language Models (LLMs):
- **Input Tokens:** Text sent to the model (prompts, context, code)
- **Output Tokens:** Text generated by the model (responses, code, documentation)
- **Total Tokens:** Input + Output

**Approximation:** ~4 characters = 1 token (English text), ~1 token per line of code

### Session Budget

The maximum token limit per conversation session:
- **Claude Sonnet 4.5:** 1,000,000 tokens (1M context window)
- **Claude Opus 4.5:** 1,000,000 tokens
- **Claude Haiku 4.5:** 200,000 tokens

**Best Practice:** Reserve 20% buffer for overhead â†’ Usable budget: 800K tokens (Sonnet/Opus), 160K tokens (Haiku)

### Model Selection Guidelines

**Quick Reference:**

| Model | Context Window | Usable Budget | Best For | Cost (Relative) |
|-------|---------------|---------------|----------|-----------------|
| **Haiku 4.5** | 200K tokens | 160K tokens | Simple, repetitive tasks | 1x (baseline) |
| **Sonnet 4.5** | 1M tokens | 800K tokens | Most tasks, good balance | ~5x |
| **Opus 4.5** | 1M tokens | 800K tokens | Complex, critical tasks | ~15x |

**Decision Rules:**
- **<20K tokens** â†’ Haiku
- **20-80K tokens** â†’ Sonnet
- **>80K tokens** â†’ Opus

**See:** [model-selection.md](model-selection.md) for complete decision criteria, task-type mapping, cost-performance trade-offs, and phase-specific recommendations.

---

## Estimation Methodology

### Phase-Based Estimation

Different phases have different token consumption patterns:

| Phase | Avg. Tokens/Task | Typical Range | Notes |
|-------|------------------|---------------|-------|
| **Phase 1: Requirements** | 5,000 - 15,000 | 3K - 30K | Specification writing, competitive research |
| **Phase 2: Planning** | 10,000 - 30,000 | 5K - 60K | Architecture design, task decomposition |
| **Phase 3: Implementation** | 15,000 - 50,000 | 8K - 100K | Code generation, debugging, testing |
| **Phase 4: Verification** | 8,000 - 20,000 | 5K - 40K | Test generation, analysis, reporting |
| **Phase 5: Release** | 5,000 - 12,000 | 3K - 25K | Documentation, retrospectives |

### Task Complexity Multipliers

Apply multipliers based on task characteristics:

| Complexity Factor | Multiplier | Description |
|-------------------|------------|-------------|
| **Simple** | 1.0x | Single file, <100 LOC, clear requirements |
| **Moderate** | 2.0x | 2-3 files, 100-300 LOC, some ambiguity |
| **Complex** | 3.5x | 4+ files, 300-700 LOC, multiple dependencies |
| **Very Complex** | 5.0x | 5+ files, 700+ LOC, architectural changes |

**Additional Multipliers:**
- **New Technology:** +1.5x (unfamiliar libraries/frameworks)
- **Legacy Code:** +1.3x (working with existing, undocumented code)
- **High Security:** +1.2x (security-critical tasks require extra validation)
- **Performance Critical:** +1.3x (optimization tasks require profiling/testing)

### Estimation Formula

```
Estimated Tokens = Base Estimate Ã— Complexity Multiplier Ã— Additional Multipliers
```

**Example:**
```
Task: Implement user authentication module (JWT)
- Base Estimate: 25,000 tokens (Phase 3 average)
- Complexity: Complex (4 files, 350 LOC) â†’ 3.5x
- High Security: +1.2x

Estimated Tokens = 25,000 Ã— 3.5 Ã— 1.2 = 105,000 tokens
```

---

## Token Budget Allocation

### Session-Level Budgets

**Per session allocation:**
```
Total Session Budget: 1,000,000 tokens (Sonnet 4.5)
- Reserve (20%): 200,000 tokens
- Usable Budget: 800,000 tokens

Recommended Allocation:
- Context Loading (15%): 120,000 tokens
- Task Execution (70%): 560,000 tokens
- Overhead/Debugging (15%): 120,000 tokens
```

### Task-Level Budgets

**Budget categories:**

| Task Type | Token Budget | Rationale |
|-----------|--------------|-----------|
| **Setup/Config** | 5,000 - 15,000 | Minimal code generation |
| **CRUD Operations** | 15,000 - 30,000 | Repetitive patterns, clear structure |
| **Business Logic** | 25,000 - 60,000 | Complex logic, edge cases |
| **API Integration** | 30,000 - 70,000 | External docs, error handling |
| **UI Components** | 20,000 - 50,000 | Styling, interactivity, accessibility |
| **Testing** | 10,000 - 35,000 | Test generation, coverage |
| **Refactoring** | 20,000 - 80,000 | Understanding + restructuring |
| **Bug Fixes** | 8,000 - 40,000 | Debugging, root cause analysis |

### Milestone-Level Budgets

**Aggregate task budgets + buffer:**
```
Milestone Budget = Î£(Task Budgets) Ã— 1.15 (15% buffer for unknowns)
```

---

## Estimation in Phase 2 (Planning)

### When to Estimate

During **Step 2.8: Task File Generation**, estimate tokens for each task:

1. **Analyze task complexity** (file count, LOC, dependencies)
2. **Apply phase baseline** (Phase 3 = 15K-50K average)
3. **Apply multipliers** (complexity, technology, security)
4. **Round to nearest 5K** (e.g., 23K â†’ 25K)
5. **Document rationale** in task file

### Estimation Template

Include in each task file:

```markdown
## Token Estimation

**Estimated Tokens:** 35,000 tokens
**Recommended Model:** Claude Sonnet 4.5

**Rationale:**
- **Base Estimate:** 25,000 tokens (Phase 3, moderate complexity)
- **Complexity:** Moderate (3 files, ~200 LOC) â†’ 2.0x
- **Multipliers:**
  - New library (Context7 integration): +1.5x
- **Calculation:** 25,000 Ã— 2.0 Ã— 1.5 = 75,000 â†’ Adjusted to 35,000 (refined based on similar past tasks)

**Breakdown:**
- Code generation: 20,000 tokens
- Testing: 8,000 tokens
- Documentation: 5,000 tokens
- Debugging buffer: 2,000 tokens

**Model Selection Rationale:**
- **35K tokens** falls in Sonnet range (20-80K)
- Moderate complexity requires balanced reasoning
- New library integration benefits from Sonnet's superior understanding
- Cost-effective for this complexity level
```

### Aggregation in Task DAG

Add token and model columns to task tables:

| Task ID | Description | Est. Hours | Est. Tokens | Model | Dependencies | AC Mapping |
|---------|-------------|-----------|-------------|-------|--------------|------------|
| T-1.1.1 | Setup project | 4 | 10K | Haiku | None | AC-1.1 |
| T-1.1.2 | Config DB | 6 | 15K | Haiku | T-1.1.1 | AC-1.2 |
| T-1.1.3 | Auth module | 8 | 35K | Sonnet | T-1.1.2 | AC-1.3 |

**Milestone Summary:**
```markdown
**Total Estimated Tokens:** 60,000 tokens

**Task Breakdown by Model:**
- Haiku tasks (2): 25K tokens â†’ Cost: ~$0.01 (estimated)
- Sonnet tasks (1): 35K tokens â†’ Cost: ~$0.04 (estimated)
- **Total Cost:** ~$0.05 (vs ~$0.18 if all Sonnet)
- **Savings:** ~72% by using Haiku for simple tasks

**Recommended Session Model:** Claude Sonnet 4.5 (primary session)
**Cost Optimization:** Switch to Haiku for T-1.1.1 and T-1.1.2
**Fits Within Budget:** âœ… Yes (60K < 800K Sonnet budget)
```

---

## Token Tracking in Phase 3 (Implementation)

### Step 3.3.1c: Session Budget and Model Check

**Before starting task execution:**

```markdown
### Session Budget & Model Check

**Session Model:** Claude Sonnet 4.5
**Session Budget:** 1,000,000 tokens (Sonnet 4.5)
- Usable: 800,000 tokens
- Current usage: 142,000 tokens (14.2%)
- Remaining: 658,000 tokens

**Current Task:** T-1.1.3 (Auth module)
**Task Token Budget:** 35,000 tokens
**Task Recommended Model:** Sonnet

**Status:** âœ… Sufficient budget (35K < 658K remaining)
**Model Match:** âœ… Task requires Sonnet, session is Sonnet

**Action:** Proceed with task execution
```

**If model mismatch (cost optimization opportunity):**
```markdown
**Current Task:** T-1.1.1 (Setup project)
**Task Token Budget:** 10,000 tokens
**Task Recommended Model:** Haiku
**Session Model:** Sonnet

**Recommendation:**
- âœ… Continue in Sonnet (already started, switching adds overhead)
- OR
- ðŸ’¡ For next session: Start with Haiku for simple tasks, switch to Sonnet for complex ones
- Potential savings: ~60% cost reduction on simple tasks
```

**If insufficient budget:**
```markdown
**Status:** âš ï¸ Low budget (only 45K remaining, need 50K)

**Recommendation:**
1. Complete current task (if <45K)
2. Create recovery checkpoint
3. Start new session with handoff
4. Continue from checkpoint
5. Consider model switch: If remaining tasks are simple, use Haiku for next session
```

### Step 3.3.7: Actual Token Recording

**After task completion:**

```markdown
## Token Usage

**Estimated:** 35,000 tokens
**Actual:** 42,300 tokens
**Variance:** +7,300 tokens (+20.9%)

**Breakdown:**
- Code generation: 24,500 tokens (est. 20,000)
- Testing: 10,200 tokens (est. 8,000)
- Documentation: 5,100 tokens (est. 5,000)
- Debugging: 2,500 tokens (est. 2,000)

**Variance Analysis:**
- Higher debugging time due to unfamiliar library API
- Testing required more edge cases than anticipated
- Documentation was on target

**Lessons Learned:**
- Increase new library multiplier from 1.5x to 1.7x
- Add +20% buffer for unfamiliar APIs
```

### Session Token Summary

**In recovery checkpoint:**
```markdown
## Session Token Metrics

**Session Model:** Claude Sonnet 4.5 (1M context)
**Session Duration:** ~3 hours
**Tasks Completed:** 5/8

**Token Usage:**
- Total Used: 142,000 tokens (14.2% of budget)
- Average per Task: 28,400 tokens
- Remaining Budget: 658,000 tokens

**Efficiency:**
- Estimated Total: 120,000 tokens
- Actual Total: 142,000 tokens
- Variance: +18.3% (within acceptable range)

**Recommendation for Next Session:**
- Continue with Claude Sonnet 4.5
- Remaining 3 tasks (est. 95K tokens) fit within budget
```

---

## Token Metrics in Phase 4 (Verification)

### Evidence Package Section

**Add to `v[X.Y.Z]-evidence.md`:**

```markdown
## Token Efficiency Metrics

**Session Summary:**
- **Model Used:** Claude Sonnet 4.5
- **Total Sessions:** 3
- **Total Tokens:** 387,000 tokens
- **Session Budget Utilization:** 38.7% (efficient)

**Task-Level Performance:**
| Milestone | Tasks | Est. Tokens | Actual Tokens | Variance |
|-----------|-------|-------------|---------------|----------|
| M1: Foundation | 8 | 120,000 | 142,000 | +18.3% |
| M2: Core Features | 12 | 180,000 | 165,000 | -8.3% |
| M3: Advanced | 6 | 90,000 | 80,000 | -11.1% |
| **Total** | **26** | **390,000** | **387,000** | **-0.8%** |

**Analysis:**
- **Overall Accuracy:** Within Â±1% (excellent)
- **Early Tasks:** Over-budget (+18%) due to learning curve
- **Later Tasks:** Under-budget (-8 to -11%) due to pattern reuse and improved prompting

**Key Insights:**
- Task complexity multipliers were accurate
- New library multiplier should be increased from 1.5x to 1.7x
- Testing token estimates were consistently accurate
- Documentation token estimates need +10% adjustment

**Cost Efficiency:**
- Estimated Cost: $11.70 (390K tokens @ $3/MTok input + output mixed)
- Actual Cost: $11.61 (387K tokens)
- Savings: $0.09 (accurate estimation prevented over-provisioning)

**Recommendations for Next Project:**
- Apply 1.7x multiplier for new libraries
- Increase documentation estimates by 10%
- Maintain current complexity multipliers (working well)
```

---

## Token Retrospective in Phase 5 (Release)

### Lessons Learned Section

**Add to `lessons-learned.md`:**

```markdown
## Token Management Lessons

### What Went Well

1. **Estimation Accuracy:**
   - Final variance: -0.8% (390K estimated vs 387K actual)
   - Milestone-level estimates were within Â±20%
   - Task-level estimates improved over time (learning effect)

2. **Session Management:**
   - Never exceeded session budget (max utilization: 42%)
   - Effective use of recovery checkpoints
   - Minimal context thrashing

3. **Cost Predictability:**
   - Accurate cost forecasting ($11.70 est. vs $11.61 actual)
   - No budget overruns
   - Enabled confident ROI planning

### What Could Improve

1. **Early Task Estimates:**
   - M1 tasks were +18% over budget
   - Learning curve not adequately accounted for
   - **Action:** Add "project startup" multiplier (1.2x) for first 3-5 tasks

2. **New Library Integration:**
   - 1.5x multiplier was insufficient
   - Actual required 1.7x-1.8x
   - **Action:** Update multiplier to 1.7x in token-estimation.md

3. **Documentation Token Estimates:**
   - Consistently under-estimated by ~10%
   - Complex projects require more thorough docs
   - **Action:** Increase documentation baseline by 10%

### Process Improvements

1. **Add Token Checkpoints:**
   - Check budget before starting each task (Step 3.3.1c) âœ…
   - Alert at 80% budget utilization
   - Recommend session break at 85%

2. **Historical Data Collection:**
   - Log all task-level actuals to knowledge base
   - Build project-specific baselines
   - Use past data to refine future estimates

3. **Real-Time Tracking:**
   - Display token usage in recovery checkpoint âœ…
   - Show variance trends during implementation
   - Provide session-end token summary

### Recommendations for Future Projects

| Recommendation | Rationale | Impact |
|----------------|-----------|--------|
| Apply 1.7x multiplier for new libs | 1.5x was insufficient | Higher estimation accuracy |
| Add 1.2x "startup" multiplier for M1 | Learning curve effect | Better early-phase estimates |
| Increase doc baseline by 10% | Consistent under-estimation | Accurate doc token budgets |
| Build project-type baselines | Domain-specific patterns | Faster, more accurate estimates |
| Log actuals to KB automatically | Enable ML-based estimation | Continuous improvement |

### Token Efficiency Score

**Overall Score:** 8.5/10

**Breakdown:**
- Estimation Accuracy (30%): 9.5/10 (within Â±1%)
- Budget Management (25%): 9.0/10 (never exceeded, minimal waste)
- Cost Predictability (20%): 9.5/10 (accurate forecasting)
- Session Optimization (15%): 7.5/10 (some sessions under-utilized)
- Learning Adaptation (10%): 7.0/10 (slow adjustment in M1)

**Interpretation:**
- **Excellent (9-10):** Estimation accuracy, cost control
- **Good (7-8):** Session optimization, learning adaptation
- **Target for Next Project:** 9.0/10 overall
```

---

## Model Selection Decision Matrix

### Task Type to Model Mapping

| Task Type | Token Range | Complexity | Recommended Model | Rationale |
|-----------|-------------|------------|-------------------|-----------|
| **Setup/Config** | 5K-15K | Simple | Haiku | Repetitive, well-defined patterns |
| **CRUD Operations** | 15K-30K | Simple-Moderate | Haiku/Sonnet* | Haiku for standard, Sonnet if custom logic |
| **Business Logic** | 25K-60K | Complex | Sonnet | Requires reasoning, edge cases |
| **API Integration** | 30K-70K | Complex | Sonnet | External docs, error handling |
| **UI Components** | 20K-50K | Moderate-Complex | Sonnet | Interactivity, accessibility, styling |
| **Testing** | 10K-35K | Moderate | Haiku/Sonnet* | Haiku for unit, Sonnet for integration |
| **Refactoring** | 20K-80K | Very Complex | Sonnet/Opus* | Sonnet for module, Opus for architectural |
| **Bug Fixes** | 8K-40K | Varies | Haiku/Sonnet* | Haiku for simple, Sonnet for complex |
| **Architecture Design** | 50K-100K+ | Very Complex | Opus | Phase 2 decisions, critical design |
| **Security Implementation** | 30K-80K | Complex-Critical | Sonnet/Opus* | Sonnet for standard, Opus for novel threats |
| **Performance Optimization** | 40K-100K+ | Very Complex | Opus | Deep analysis, profiling, novel solutions |
| **Documentation** | 5K-20K | Simple-Moderate | Haiku | Well-structured, template-based |

*Choose based on specific characteristics described in the Rationale column

### Phase-Specific Model Recommendations

| Phase | Primary Model | Alternative | When to Use Alternative |
|-------|---------------|-------------|------------------------|
| **Phase 1: Requirements** | Sonnet | Opus | Highly complex domain, novel product category |
| **Phase 2: Planning** | Opus | Sonnet | Standard patterns (Sonnet), Novel architecture (Opus) |
| **Phase 3: Implementation** | Sonnet | Haiku/Opus | Simple tasks (Haiku), Critical/novel code (Opus) |
| **Phase 4: Verification** | Sonnet | Opus | Standard verification (Sonnet), Complex debugging (Opus) |
| **Phase 5: Release** | Sonnet | - | Lessons learned, documentation |

### Cost-Performance Trade-offs

**Scenario 1: Simple CRUD Application**
```
Total Tasks: 20
- Setup/Config (5 tasks): Haiku â†’ 50K tokens @ $0.02 = $0.02
- CRUD Operations (12 tasks): Haiku â†’ 300K tokens @ $0.02/10K = $0.60
- Business Logic (3 tasks): Sonnet â†’ 120K tokens @ $0.10/10K = $1.20
**Total Cost:** $1.82 (vs $4.50 if all Sonnet) â†’ 60% savings
```

**Scenario 2: Complex AI/ML Platform**
```
Total Tasks: 30
- Infrastructure (8 tasks): Haiku â†’ 100K tokens @ $0.02/10K = $0.20
- Model Integration (15 tasks): Sonnet â†’ 600K tokens @ $0.10/10K = $6.00
- Novel Algorithms (5 tasks): Opus â†’ 400K tokens @ $0.40/10K = $16.00
- Verification (2 tasks): Opus â†’ 150K tokens @ $0.40/10K = $6.00
**Total Cost:** $28.20 (justified for complex domain)
```

**Cost Optimization Strategies:**

1. **Batch Simple Tasks:** Use Haiku session for multiple setup/config tasks
2. **Session Planning:** Start Haiku â†’ Switch to Sonnet â†’ Use Opus sparingly
3. **Model Switching:** Create checkpoints when changing models
4. **Parallel Work:** Use Haiku for docs while Sonnet handles logic
5. **Learn from Past:** If Haiku struggles, upgrade to Sonnet for similar future tasks

### Model Selection Examples

#### Example 1: Authentication Module (T-2.1.3)

**Task:** Implement JWT authentication with refresh tokens

**Analysis:**
- **Files:** 3 (controller, service, middleware)
- **LOC:** ~250 lines
- **Complexity:** Complex (security-critical, edge cases)
- **Token Estimate:** 35,000 tokens
- **New Technology:** NestJS (team learning)

**Model Decision:** **Sonnet**

**Rationale:**
- 35K tokens fits Sonnet range (20-80K)
- Security-critical (but not novel threat â†’ not Opus)
- New technology benefits from Sonnet's understanding
- Cost-effective balance ($0.35 vs $1.40 for Opus)

#### Example 2: Database Setup Script (T-1.1.2)

**Task:** Create PostgreSQL initialization script with seed data

**Analysis:**
- **Files:** 1 (init.sql)
- **LOC:** ~80 lines
- **Complexity:** Simple (standard patterns)
- **Token Estimate:** 12,000 tokens
- **Pattern:** Well-defined SQL schema

**Model Decision:** **Haiku**

**Rationale:**
- 12K tokens fits Haiku range (<20K)
- Repetitive SQL patterns (CREATE TABLE, INSERT)
- No novel logic required
- Cost savings: $0.02 vs $0.12 (Sonnet) â†’ 83% reduction

#### Example 3: Real-time Sync Algorithm (T-3.2.5)

**Task:** Design and implement real-time data synchronization algorithm for offline-first mobile app

**Analysis:**
- **Files:** 4-5 (sync engine, conflict resolution, queue manager)
- **LOC:** ~600 lines
- **Complexity:** Very Complex (novel algorithm, edge cases)
- **Token Estimate:** 95,000 tokens
- **Innovation:** Custom conflict resolution strategy

**Model Decision:** **Opus**

**Rationale:**
- 95K tokens exceeds Sonnet comfort zone (>80K)
- Novel algorithm design (not standard patterns)
- Critical to app functionality (offline-first core)
- Complex edge cases (network partitions, conflicts)
- Opus reasoning depth justifies 3x cost

---

## Commands Reference

### `/estimate` - Estimate Token Usage

**Usage:**
```bash
/estimate task T-1.2.3          # Estimate specific task
/estimate milestone M2          # Estimate entire milestone
/estimate session               # Check current session budget status
```

**Output:**
```
Task T-1.2.3: Implement Authentication Module

Estimated Tokens: 35,000 tokens

Breakdown:
- Code generation: 20,000 tokens
- Testing: 8,000 tokens
- Documentation: 5,000 tokens
- Debugging buffer: 2,000 tokens

Complexity: Complex (3.5x)
Multipliers: High Security (+1.2x)

Session Impact:
- Current usage: 142,000 tokens
- After task: ~177,000 tokens (17.7% of budget)
- Status: âœ… Sufficient budget
```

### `/budget` - Check Session Budget

**Usage:**
```bash
/budget                         # Show current session status
/budget --detailed              # Include task-level breakdown
/budget --forecast              # Forecast remaining tasks
```

**Output:**
```
Session Budget Status

Model: Claude Sonnet 4.5 (1M context)
Duration: 2h 15m

Token Usage:
- Used: 142,000 tokens (14.2%)
- Reserved: 200,000 tokens (20%)
- Available: 658,000 tokens (65.8%)

Remaining Tasks: 3
- T-2.1.1: 25,000 tokens (est.)
- T-2.1.2: 30,000 tokens (est.)
- T-2.1.3: 40,000 tokens (est.)
- Total: 95,000 tokens

Forecast:
- After completion: 237,000 tokens used (23.7%)
- Status: âœ… All tasks fit within budget
- Recommendation: Continue in current session
```

### `/variance` - Analyze Token Variance

**Usage:**
```bash
/variance                       # Overall variance analysis
/variance milestone M2          # Milestone-specific analysis
/variance task T-1.2.3          # Task-specific analysis
```

**Output:**
```
Token Variance Analysis

Overall:
- Estimated: 390,000 tokens
- Actual: 387,000 tokens
- Variance: -3,000 tokens (-0.8%)
- Accuracy: Excellent âœ…

By Milestone:
M1: +18.3% (over-budget, learning curve)
M2: -8.3% (under-budget, pattern reuse)
M3: -11.1% (under-budget, optimization)

By Phase:
Phase 2 (Planning): +5.2%
Phase 3 (Implementation): -2.1%
Phase 4 (Verification): -4.5%

Trends:
- Improving accuracy over time âœ…
- Early estimates too conservative
- Testing estimates accurate
- Documentation under-estimated by ~10%

Recommendations:
1. Apply 1.7x multiplier for new libraries
2. Add 10% to documentation baselines
3. Use historical data for future projects
```

---

## Best Practices

### For Software Architects (Phase 2)

1. **Estimate Early:** Include token estimates in task decomposition
2. **Document Rationale:** Explain complexity factors and multipliers
3. **Aggregate Wisely:** Add 15% buffer at milestone level
4. **Review Historical Data:** Check knowledge base for similar tasks
5. **Communicate Budgets:** Make token budgets visible in task files

### For Senior Developers (Phase 3)

1. **Check Budget First:** Run `/budget` before starting each task (Step 3.3.1c)
2. **Track Actuals:** Record actual token usage after task completion (Step 3.3.7)
3. **Analyze Variance:** Understand why estimates differed from actuals
4. **Update Checkpoints:** Include token metrics in recovery checkpoints
5. **Optimize Prompts:** Reuse patterns to reduce token consumption

### For QA Leads (Phase 4)

1. **Include Token Metrics:** Add token analysis to evidence package
2. **Evaluate Efficiency:** Assess token efficiency as quality metric
3. **Identify Trends:** Spot patterns in token usage across milestones
4. **Calculate ROI:** Estimate cost savings from accurate estimation
5. **Document Learnings:** Capture insights for future projects

### For Release Managers (Phase 5)

1. **Retrospective Analysis:** Include token lessons in lessons-learned
2. **Update Knowledge Base:** Log actual vs estimated for pattern recognition
3. **Refine Multipliers:** Adjust estimation formulas based on outcomes
4. **Publish Benchmarks:** Share token efficiency metrics with team
5. **Plan Future Budgets:** Use actuals to inform next project estimates

---

## Token Optimization Strategies

### Code Reuse
- **Pattern Libraries:** Reuse code patterns from knowledge base (-30% tokens)
- **Template Usage:** Leverage CodeMaestro templates (-40% tokens)
- **Component Reuse:** Reference existing modules instead of regenerating

### Context Management
- **Lazy Loading:** Load templates only when needed (-50% context tokens)
- **Progressive Disclosure:** Use on-demand documentation loading
- **Session Splitting:** Break at logical boundaries to reset context

### Prompt Engineering
- **Specific Instructions:** Reduce back-and-forth iterations (-20% tokens)
- **Constraint References:** Use IDs (A1, B15) instead of full text (-60% tokens)
- **Template References:** Link to templates instead of inlining (-70% tokens)

### Knowledge Base Usage
- **Learn from Past:** Reference previous solutions (-25% tokens)
- **Pattern Matching:** Apply proven approaches (-35% tokens)
- **Failure Avoidance:** Learn from logged failures (-40% debugging tokens)

---

## Troubleshooting

### Issue: Token Estimates Consistently Too Low

**Symptoms:**
- Variance > +25% across multiple tasks
- Session budget exhausted before completion

**Root Causes:**
1. Complexity multipliers too conservative
2. New technology learning curve not accounted for
3. Debugging buffer insufficient

**Solutions:**
- Increase complexity multipliers by 0.5x
- Add "new technology" multiplier (1.7x)
- Increase debugging buffer from 10% to 20%

### Issue: Token Estimates Consistently Too High

**Symptoms:**
- Variance < -20% across multiple tasks
- Session budget under-utilized (<40%)

**Root Causes:**
1. Over-conservative baseline estimates
2. Efficient prompt engineering reducing usage
3. Pattern reuse from knowledge base

**Solutions:**
- Reduce baseline estimates by 15%
- Adjust complexity multipliers down by 0.3x
- Document efficiency gains in KB for future reference

### Issue: Session Budget Exceeded Mid-Task

**Symptoms:**
- Task incomplete, session context limit reached
- Unable to complete current work

**Solutions:**
1. **Immediate:** Create recovery checkpoint with current progress
2. **Immediate:** Document partial completion in task file
3. **Next Session:** Resume from checkpoint with fresh budget
4. **Future:** Increase task buffer from 15% to 25% for complex tasks

---

## Appendix A: Token Estimation Cheat Sheet

| Task Type | Base Tokens | Complexity | Multipliers |
|-----------|-------------|------------|-------------|
| Setup/Config | 5K - 15K | Simple (1.0x) | - |
| CRUD Operations | 15K - 30K | Moderate (2.0x) | - |
| Business Logic | 25K - 60K | Complex (3.5x) | High Security (+1.2x) |
| API Integration | 30K - 70K | Complex (3.5x) | New Library (+1.7x) |
| UI Components | 20K - 50K | Moderate (2.0x) | Accessibility (+1.1x) |
| Testing | 10K - 35K | Moderate (2.0x) | - |
| Refactoring | 20K - 80K | Very Complex (5.0x) | Legacy Code (+1.3x) |
| Bug Fixes | 8K - 40K | Varies | - |

**Quick Formula:**
```
Tokens = Base Ã— Complexity Ã— (1 + Î£ Multiplier%)
```

---

## Appendix B: Historical Token Data Format

**Store in knowledge base:**
```markdown
# Task Token History

## T-1.2.3: Implement Authentication Module

**Project:** Project Alpha
**Date:** 2026-01-15
**Phase:** 3 (Implementation)

**Estimate:** 35,000 tokens
**Actual:** 42,300 tokens
**Variance:** +20.9%

**Characteristics:**
- Files: 3 (auth-controller.ts, auth-service.ts, auth-middleware.ts)
- LOC: 285
- Complexity: Complex (3.5x)
- Technology: NestJS (new to team, +1.5x multiplier - should have been 1.7x)
- Security: High (+1.2x)

**Breakdown:**
- Code generation: 24,500 tokens (20,000 est.) - +22.5%
- Testing: 10,200 tokens (8,000 est.) - +27.5%
- Documentation: 5,100 tokens (5,000 est.) - +2%
- Debugging: 2,500 tokens (2,000 est.) - +25%

**Lessons:**
- New library multiplier should be 1.7x (not 1.5x)
- Testing required more edge cases (JWT expiration, refresh tokens)
- Debugging took longer due to passport.js integration issues

**Tags:** #authentication #jwt #nestjs #new-library #complex
```

---

## Version

**Token Estimation Guide Version:** 1.0
**CodeMaestro:** v1.0.0
**Last Updated:** 2026-01-16
